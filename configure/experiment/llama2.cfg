[model]
name = unified.finetune
# TODO
use_description = False
# TODO
concatenate_description = False
# Should be one of (separate, concatenate)
knowledge_usage = concatenate
external_knowledge = concatenate

[dataset]
data_store_path = ./data
# TODO
#eval_num = 500
is_eval = False
# Larger upsample_temp leads to more uniform sampling
upsample_temp = 5

[seq2seq]
constructor = seq2seq_construction.meta_tuning
model_type = DecoderOnly
schema_serialization_customized = True

[arg_paths]
bird = META_TUNING/bird.cfg

[evaluate]
tool = metrics.meta_tuning.evaluator

[special_tokens]
less = ' <'
less_or_equal = ' <='

[bert]
#location = tscholak/t5.1.1.lm100k.large
location = /opt/data/private/hf_models/SQL_Llama-7B